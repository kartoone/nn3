{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOrU98/mJSwHNX2chOnfl74",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kartoone/nn3/blob/main/colab/theano2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!cat ~/.keras/keras.json\n",
        "!git clone \"https://github.com/kartoone/nn3\"\n",
        "!cat nn3/keras.json > ~/.keras/keras.json\n",
        "!pip uninstall -y keras\n",
        "!pip uninstall -y tensorflow\n",
        "!pip install tensorflow==2.2\n",
        "!pip install keras==2.2.4\n",
        "!pip install pydot-ng\n",
        "!pip install theano==0.8\n",
        "%cd nn3/src\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zufu4wflP581",
        "outputId": "34246668-b3fe-4c5d-f70a-a973ca877bbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nn3'...\n",
            "remote: Enumerating objects: 166, done.\u001b[K\n",
            "remote: Counting objects: 100% (57/57), done.\u001b[K\n",
            "remote: Compressing objects: 100% (44/44), done.\u001b[K\n",
            "remote: Total 166 (delta 31), reused 26 (delta 11), pack-reused 109\u001b[K\n",
            "Receiving objects: 100% (166/166), 94.35 MiB | 19.71 MiB/s, done.\n",
            "Resolving deltas: 100% (40/40), done.\n",
            "Updating files: 100% (106/106), done.\n",
            "Found existing installation: keras 2.9.0\n",
            "Uninstalling keras-2.9.0:\n",
            "  Successfully uninstalled keras-2.9.0\n",
            "Found existing installation: tensorflow 2.9.2\n",
            "Uninstalling tensorflow-2.9.2:\n",
            "  Successfully uninstalled tensorflow-2.9.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==2.2\n",
            "  Downloading tensorflow-2.2.0-cp38-cp38-manylinux2010_x86_64.whl (516.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m516.3/516.3 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.2) (1.21.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.2) (3.3.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.2) (1.6.3)\n",
            "Collecting scipy==1.4.1\n",
            "  Downloading scipy-1.4.1-cp38-cp38-manylinux1_x86_64.whl (26.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.0/26.0 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.2) (0.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.2) (1.14.1)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.2) (3.19.6)\n",
            "Collecting tensorflow-estimator<2.3.0,>=2.2.0\n",
            "  Downloading tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.6/454.6 KB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.2) (1.1.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.2) (0.38.4)\n",
            "Collecting tensorboard<2.3.0,>=2.2.0\n",
            "  Downloading tensorboard-2.2.2-py3-none-any.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.2) (1.51.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.2) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.2) (2.2.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.2) (1.4.0)\n",
            "Collecting gast==0.3.3\n",
            "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
            "Collecting h5py<2.11.0,>=2.10.0\n",
            "  Downloading h5py-2.10.0-cp38-cp38-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (1.0.1)\n",
            "Collecting google-auth<2,>=1.6.3\n",
            "  Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.9/152.9 KB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (1.8.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (57.4.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (2.25.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (4.9)\n",
            "Collecting cachetools<5.0,>=2.0.0\n",
            "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (6.0.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (2022.12.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (3.12.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (3.2.2)\n",
            "Installing collected packages: tensorflow-estimator, scipy, h5py, gast, cachetools, google-auth, tensorboard, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.9.0\n",
            "    Uninstalling tensorflow-estimator-2.9.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.9.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.7.3\n",
            "    Uninstalling scipy-1.7.3:\n",
            "      Successfully uninstalled scipy-1.7.3\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Attempting uninstall: cachetools\n",
            "    Found existing installation: cachetools 5.3.0\n",
            "    Uninstalling cachetools-5.3.0:\n",
            "      Successfully uninstalled cachetools-5.3.0\n",
            "  Attempting uninstall: google-auth\n",
            "    Found existing installation: google-auth 2.16.0\n",
            "    Uninstalling google-auth-2.16.0:\n",
            "      Successfully uninstalled google-auth-2.16.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.9.1\n",
            "    Uninstalling tensorboard-2.9.1:\n",
            "      Successfully uninstalled tensorboard-2.9.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "keras-vis 0.4.1 requires keras, which is not installed.\n",
            "xarray-einstats 0.5.1 requires scipy>=1.6, but you have scipy 1.4.1 which is incompatible.\n",
            "plotnine 0.8.0 requires scipy>=1.5.0, but you have scipy 1.4.1 which is incompatible.\n",
            "jaxlib 0.3.25+cuda11.cudnn805 requires scipy>=1.5, but you have scipy 1.4.1 which is incompatible.\n",
            "jax 0.3.25 requires scipy>=1.5, but you have scipy 1.4.1 which is incompatible.\n",
            "google-api-core 2.11.0 requires google-auth<3.0dev,>=2.14.1, but you have google-auth 1.35.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed cachetools-4.2.4 gast-0.3.3 google-auth-1.35.0 h5py-2.10.0 scipy-1.4.1 tensorboard-2.2.2 tensorflow-2.2.0 tensorflow-estimator-2.2.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras==2.2.4\n",
            "  Downloading Keras-2.2.4-py2.py3-none-any.whl (312 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m312.5/312.5 KB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras-applications>=1.0.6\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 KB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.8/dist-packages (from keras==2.2.4) (1.4.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from keras==2.2.4) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.8/dist-packages (from keras==2.2.4) (1.21.6)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.8/dist-packages (from keras==2.2.4) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from keras==2.2.4) (6.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.8/dist-packages (from keras==2.2.4) (1.1.2)\n",
            "Installing collected packages: keras-applications, keras\n",
            "Successfully installed keras-2.2.4 keras-applications-1.0.8\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pydot-ng in /usr/local/lib/python3.8/dist-packages (2.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from pydot-ng) (3.0.9)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting theano==0.8\n",
            "  Downloading Theano-0.8.0.zip (3.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.7.1 in /usr/local/lib/python3.8/dist-packages (from theano==0.8) (1.21.6)\n",
            "Requirement already satisfied: scipy>=0.11 in /usr/local/lib/python3.8/dist-packages (from theano==0.8) (1.4.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from theano==0.8) (1.15.0)\n",
            "Building wheels for collected packages: theano\n",
            "  Building wheel for theano (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for theano: filename=Theano-0.8.0-py3-none-any.whl size=2722137 sha256=dda924395268a7243cbf9d316e830b47d64587e20bc4314333434d68561e189f\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/fa/00/251dbd5e561228175439939ac67c8c17484f615a245c2479d8\n",
            "Successfully built theano\n",
            "Installing collected packages: theano\n",
            "Successfully installed theano-0.8.0\n",
            "/content/nn3/src\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat ~/.keras/keras.json\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apRaTZgACf0m",
        "outputId": "7005d8c3-9515-4bd6-9a1f-aa7f607c524c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"floatx\": \"float32\",\n",
            "    \"epsilon\": 1e-07,\n",
            "    \"backend\": \"theano\",\n",
            "    \"image_dim_ordering\": \"th\",\n",
            "    \"image_data_format\": \"channels_first\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import network3\n",
        "from network3 import Network\n",
        "from network3 import ConvPoolLayer, FullyConnectedLayer, SoftmaxLayer\n",
        "training_data, validation_data, test_data = network3.load_data_shared()\n",
        "mini_batch_size = 10\n",
        "net = Network([\n",
        "        FullyConnectedLayer(n_in=784, n_out=100),\n",
        "        SoftmaxLayer(n_in=100, n_out=10)], mini_batch_size)\n",
        "net.SGD(training_data, 60, mini_batch_size, 0.1, \n",
        "            validation_data, test_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DgOjEgKjVp0o",
        "outputId": "5ee30b08-fd2b-41e8-ef1e-12f120d4c3bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trying to run under a GPU.  If this is not desired, then modify network3.py\n",
            "to set the GPU flag to False.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING (theano.tensor.blas): We did not found a dynamic library into the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.\n",
            "WARNING:theano.tensor.blas:We did not found a dynamic library into the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training mini-batch number 0\n",
            "Training mini-batch number 1000\n",
            "Training mini-batch number 2000\n",
            "Training mini-batch number 3000\n",
            "Training mini-batch number 4000\n",
            "Epoch 0: validation accuracy 0.9262\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9166000000000001\n",
            "Training mini-batch number 5000\n",
            "Training mini-batch number 6000\n",
            "Training mini-batch number 7000\n",
            "Training mini-batch number 8000\n",
            "Training mini-batch number 9000\n",
            "Epoch 1: validation accuracy 0.9469000000000001\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9411000000000002\n",
            "Training mini-batch number 10000\n",
            "Training mini-batch number 11000\n",
            "Training mini-batch number 12000\n",
            "Training mini-batch number 13000\n",
            "Training mini-batch number 14000\n",
            "Epoch 2: validation accuracy 0.9563000000000001\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9513\n",
            "Training mini-batch number 15000\n",
            "Training mini-batch number 16000\n",
            "Training mini-batch number 17000\n",
            "Training mini-batch number 18000\n",
            "Training mini-batch number 19000\n",
            "Epoch 3: validation accuracy 0.9612999999999999\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9571000000000002\n",
            "Training mini-batch number 20000\n",
            "Training mini-batch number 21000\n",
            "Training mini-batch number 22000\n",
            "Training mini-batch number 23000\n",
            "Training mini-batch number 24000\n",
            "Epoch 4: validation accuracy 0.9654\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9614000000000001\n",
            "Training mini-batch number 25000\n",
            "Training mini-batch number 26000\n",
            "Training mini-batch number 27000\n",
            "Training mini-batch number 28000\n",
            "Training mini-batch number 29000\n",
            "Epoch 5: validation accuracy 0.967\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9635\n",
            "Training mini-batch number 30000\n",
            "Training mini-batch number 31000\n",
            "Training mini-batch number 32000\n",
            "Training mini-batch number 33000\n",
            "Training mini-batch number 34000\n",
            "Epoch 6: validation accuracy 0.9683\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9652999999999999\n",
            "Training mini-batch number 35000\n",
            "Training mini-batch number 36000\n",
            "Training mini-batch number 37000\n",
            "Training mini-batch number 38000\n",
            "Training mini-batch number 39000\n",
            "Epoch 7: validation accuracy 0.97\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.967\n",
            "Training mini-batch number 40000\n",
            "Training mini-batch number 41000\n",
            "Training mini-batch number 42000\n",
            "Training mini-batch number 43000\n",
            "Training mini-batch number 44000\n",
            "Epoch 8: validation accuracy 0.9701000000000002\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9688000000000001\n",
            "Training mini-batch number 45000\n",
            "Training mini-batch number 46000\n",
            "Training mini-batch number 47000\n",
            "Training mini-batch number 48000\n",
            "Training mini-batch number 49000\n",
            "Epoch 9: validation accuracy 0.9709000000000001\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9696000000000001\n",
            "Training mini-batch number 50000\n",
            "Training mini-batch number 51000\n",
            "Training mini-batch number 52000\n",
            "Training mini-batch number 53000\n",
            "Training mini-batch number 54000\n",
            "Epoch 10: validation accuracy 0.9717\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9707\n",
            "Training mini-batch number 55000\n",
            "Training mini-batch number 56000\n",
            "Training mini-batch number 57000\n",
            "Training mini-batch number 58000\n",
            "Training mini-batch number 59000\n",
            "Epoch 11: validation accuracy 0.9721000000000002\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9715000000000001\n",
            "Training mini-batch number 60000\n",
            "Training mini-batch number 61000\n",
            "Training mini-batch number 62000\n",
            "Training mini-batch number 63000\n",
            "Training mini-batch number 64000\n",
            "Epoch 12: validation accuracy 0.9725\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9715000000000001\n",
            "Training mini-batch number 65000\n",
            "Training mini-batch number 66000\n",
            "Training mini-batch number 67000\n",
            "Training mini-batch number 68000\n",
            "Training mini-batch number 69000\n",
            "Epoch 13: validation accuracy 0.9729\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9722000000000001\n",
            "Training mini-batch number 70000\n",
            "Training mini-batch number 71000\n",
            "Training mini-batch number 72000\n",
            "Training mini-batch number 73000\n",
            "Training mini-batch number 74000\n",
            "Epoch 14: validation accuracy 0.9735\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9727\n",
            "Training mini-batch number 75000\n",
            "Training mini-batch number 76000\n",
            "Training mini-batch number 77000\n",
            "Training mini-batch number 78000\n",
            "Training mini-batch number 79000\n",
            "Epoch 15: validation accuracy 0.9741000000000002\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9731000000000002\n",
            "Training mini-batch number 80000\n",
            "Training mini-batch number 81000\n",
            "Training mini-batch number 82000\n",
            "Training mini-batch number 83000\n",
            "Training mini-batch number 84000\n",
            "Epoch 16: validation accuracy 0.9743\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9739000000000001\n",
            "Training mini-batch number 85000\n",
            "Training mini-batch number 86000\n",
            "Training mini-batch number 87000\n",
            "Training mini-batch number 88000\n",
            "Training mini-batch number 89000\n",
            "Epoch 17: validation accuracy 0.9745999999999999\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9741000000000001\n",
            "Training mini-batch number 90000\n",
            "Training mini-batch number 91000\n",
            "Training mini-batch number 92000\n",
            "Training mini-batch number 93000\n",
            "Training mini-batch number 94000\n",
            "Epoch 18: validation accuracy 0.9749000000000001\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9747\n",
            "Training mini-batch number 95000\n",
            "Training mini-batch number 96000\n",
            "Training mini-batch number 97000\n",
            "Training mini-batch number 98000\n",
            "Training mini-batch number 99000\n",
            "Epoch 19: validation accuracy 0.9752000000000001\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9747\n",
            "Training mini-batch number 100000\n",
            "Training mini-batch number 101000\n",
            "Training mini-batch number 102000\n",
            "Training mini-batch number 103000\n",
            "Training mini-batch number 104000\n",
            "Epoch 20: validation accuracy 0.9756\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9749000000000001\n",
            "Training mini-batch number 105000\n",
            "Training mini-batch number 106000\n",
            "Training mini-batch number 107000\n",
            "Training mini-batch number 108000\n",
            "Training mini-batch number 109000\n",
            "Epoch 21: validation accuracy 0.9761000000000002\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.975\n",
            "Training mini-batch number 110000\n",
            "Training mini-batch number 111000\n",
            "Training mini-batch number 112000\n",
            "Training mini-batch number 113000\n",
            "Training mini-batch number 114000\n",
            "Epoch 22: validation accuracy 0.9766000000000001\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9753000000000001\n",
            "Training mini-batch number 115000\n",
            "Training mini-batch number 116000\n",
            "Training mini-batch number 117000\n",
            "Training mini-batch number 118000\n",
            "Training mini-batch number 119000\n",
            "Epoch 23: validation accuracy 0.9767\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9756\n",
            "Training mini-batch number 120000\n",
            "Training mini-batch number 121000\n",
            "Training mini-batch number 122000\n",
            "Training mini-batch number 123000\n",
            "Training mini-batch number 124000\n",
            "Epoch 24: validation accuracy 0.977\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9755\n",
            "Training mini-batch number 125000\n",
            "Training mini-batch number 126000\n",
            "Training mini-batch number 127000\n",
            "Training mini-batch number 128000\n",
            "Training mini-batch number 129000\n",
            "Epoch 25: validation accuracy 0.9769000000000001\n",
            "Training mini-batch number 130000\n",
            "Training mini-batch number 131000\n",
            "Training mini-batch number 132000\n",
            "Training mini-batch number 133000\n",
            "Training mini-batch number 134000\n",
            "Epoch 26: validation accuracy 0.9769000000000001\n",
            "Training mini-batch number 135000\n",
            "Training mini-batch number 136000\n",
            "Training mini-batch number 137000\n",
            "Training mini-batch number 138000\n",
            "Training mini-batch number 139000\n",
            "Epoch 27: validation accuracy 0.977\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9759000000000001\n",
            "Training mini-batch number 140000\n",
            "Training mini-batch number 141000\n",
            "Training mini-batch number 142000\n",
            "Training mini-batch number 143000\n",
            "Training mini-batch number 144000\n",
            "Epoch 28: validation accuracy 0.9772000000000001\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9762000000000001\n",
            "Training mini-batch number 145000\n",
            "Training mini-batch number 146000\n",
            "Training mini-batch number 147000\n",
            "Training mini-batch number 148000\n",
            "Training mini-batch number 149000\n",
            "Epoch 29: validation accuracy 0.9771000000000002\n",
            "Training mini-batch number 150000\n",
            "Training mini-batch number 151000\n",
            "Training mini-batch number 152000\n",
            "Training mini-batch number 153000\n",
            "Training mini-batch number 154000\n",
            "Epoch 30: validation accuracy 0.9770000000000001\n",
            "Training mini-batch number 155000\n",
            "Training mini-batch number 156000\n",
            "Training mini-batch number 157000\n",
            "Training mini-batch number 158000\n",
            "Training mini-batch number 159000\n",
            "Epoch 31: validation accuracy 0.9770000000000001\n",
            "Training mini-batch number 160000\n",
            "Training mini-batch number 161000\n",
            "Training mini-batch number 162000\n",
            "Training mini-batch number 163000\n",
            "Training mini-batch number 164000\n",
            "Epoch 32: validation accuracy 0.9774\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9769000000000001\n",
            "Training mini-batch number 165000\n",
            "Training mini-batch number 166000\n",
            "Training mini-batch number 167000\n",
            "Training mini-batch number 168000\n",
            "Training mini-batch number 169000\n",
            "Epoch 33: validation accuracy 0.9772000000000001\n",
            "Training mini-batch number 170000\n",
            "Training mini-batch number 171000\n",
            "Training mini-batch number 172000\n",
            "Training mini-batch number 173000\n",
            "Training mini-batch number 174000\n",
            "Epoch 34: validation accuracy 0.9773000000000001\n",
            "Training mini-batch number 175000\n",
            "Training mini-batch number 176000\n",
            "Training mini-batch number 177000\n",
            "Training mini-batch number 178000\n",
            "Training mini-batch number 179000\n",
            "Epoch 35: validation accuracy 0.977\n",
            "Training mini-batch number 180000\n",
            "Training mini-batch number 181000\n",
            "Training mini-batch number 182000\n",
            "Training mini-batch number 183000\n",
            "Training mini-batch number 184000\n",
            "Epoch 36: validation accuracy 0.977\n",
            "Training mini-batch number 185000\n",
            "Training mini-batch number 186000\n",
            "Training mini-batch number 187000\n",
            "Training mini-batch number 188000\n",
            "Training mini-batch number 189000\n",
            "Epoch 37: validation accuracy 0.977\n",
            "Training mini-batch number 190000\n",
            "Training mini-batch number 191000\n",
            "Training mini-batch number 192000\n",
            "Training mini-batch number 193000\n",
            "Training mini-batch number 194000\n",
            "Epoch 38: validation accuracy 0.977\n",
            "Training mini-batch number 195000\n",
            "Training mini-batch number 196000\n",
            "Training mini-batch number 197000\n",
            "Training mini-batch number 198000\n",
            "Training mini-batch number 199000\n",
            "Epoch 39: validation accuracy 0.9773\n",
            "Training mini-batch number 200000\n",
            "Training mini-batch number 201000\n",
            "Training mini-batch number 202000\n",
            "Training mini-batch number 203000\n",
            "Training mini-batch number 204000\n",
            "Epoch 40: validation accuracy 0.9772000000000001\n",
            "Training mini-batch number 205000\n",
            "Training mini-batch number 206000\n",
            "Training mini-batch number 207000\n",
            "Training mini-batch number 208000\n",
            "Training mini-batch number 209000\n",
            "Epoch 41: validation accuracy 0.9773000000000001\n",
            "Training mini-batch number 210000\n",
            "Training mini-batch number 211000\n",
            "Training mini-batch number 212000\n",
            "Training mini-batch number 213000\n",
            "Training mini-batch number 214000\n",
            "Epoch 42: validation accuracy 0.9772000000000001\n",
            "Training mini-batch number 215000\n",
            "Training mini-batch number 216000\n",
            "Training mini-batch number 217000\n",
            "Training mini-batch number 218000\n",
            "Training mini-batch number 219000\n",
            "Epoch 43: validation accuracy 0.977\n",
            "Training mini-batch number 220000\n",
            "Training mini-batch number 221000\n",
            "Training mini-batch number 222000\n",
            "Training mini-batch number 223000\n",
            "Training mini-batch number 224000\n",
            "Epoch 44: validation accuracy 0.9771\n",
            "Training mini-batch number 225000\n",
            "Training mini-batch number 226000\n",
            "Training mini-batch number 227000\n",
            "Training mini-batch number 228000\n",
            "Training mini-batch number 229000\n",
            "Epoch 45: validation accuracy 0.9771\n",
            "Training mini-batch number 230000\n",
            "Training mini-batch number 231000\n",
            "Training mini-batch number 232000\n",
            "Training mini-batch number 233000\n",
            "Training mini-batch number 234000\n",
            "Epoch 46: validation accuracy 0.977\n",
            "Training mini-batch number 235000\n",
            "Training mini-batch number 236000\n",
            "Training mini-batch number 237000\n",
            "Training mini-batch number 238000\n",
            "Training mini-batch number 239000\n",
            "Epoch 47: validation accuracy 0.9769000000000001\n",
            "Training mini-batch number 240000\n",
            "Training mini-batch number 241000\n",
            "Training mini-batch number 242000\n",
            "Training mini-batch number 243000\n",
            "Training mini-batch number 244000\n",
            "Epoch 48: validation accuracy 0.9768000000000001\n",
            "Training mini-batch number 245000\n",
            "Training mini-batch number 246000\n",
            "Training mini-batch number 247000\n",
            "Training mini-batch number 248000\n",
            "Training mini-batch number 249000\n",
            "Epoch 49: validation accuracy 0.9768000000000001\n",
            "Training mini-batch number 250000\n",
            "Training mini-batch number 251000\n",
            "Training mini-batch number 252000\n",
            "Training mini-batch number 253000\n",
            "Training mini-batch number 254000\n",
            "Epoch 50: validation accuracy 0.9768000000000001\n",
            "Training mini-batch number 255000\n",
            "Training mini-batch number 256000\n",
            "Training mini-batch number 257000\n",
            "Training mini-batch number 258000\n",
            "Training mini-batch number 259000\n",
            "Epoch 51: validation accuracy 0.977\n",
            "Training mini-batch number 260000\n",
            "Training mini-batch number 261000\n",
            "Training mini-batch number 262000\n",
            "Training mini-batch number 263000\n",
            "Training mini-batch number 264000\n",
            "Epoch 52: validation accuracy 0.977\n",
            "Training mini-batch number 265000\n",
            "Training mini-batch number 266000\n",
            "Training mini-batch number 267000\n",
            "Training mini-batch number 268000\n",
            "Training mini-batch number 269000\n",
            "Epoch 53: validation accuracy 0.9771\n",
            "Training mini-batch number 270000\n",
            "Training mini-batch number 271000\n",
            "Training mini-batch number 272000\n",
            "Training mini-batch number 273000\n",
            "Training mini-batch number 274000\n",
            "Epoch 54: validation accuracy 0.9772000000000001\n",
            "Training mini-batch number 275000\n",
            "Training mini-batch number 276000\n",
            "Training mini-batch number 277000\n",
            "Training mini-batch number 278000\n",
            "Training mini-batch number 279000\n",
            "Epoch 55: validation accuracy 0.9774\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9771000000000002\n",
            "Training mini-batch number 280000\n",
            "Training mini-batch number 281000\n",
            "Training mini-batch number 282000\n",
            "Training mini-batch number 283000\n",
            "Training mini-batch number 284000\n",
            "Epoch 56: validation accuracy 0.9772000000000001\n",
            "Training mini-batch number 285000\n",
            "Training mini-batch number 286000\n",
            "Training mini-batch number 287000\n",
            "Training mini-batch number 288000\n",
            "Training mini-batch number 289000\n",
            "Epoch 57: validation accuracy 0.9773000000000001\n",
            "Training mini-batch number 290000\n",
            "Training mini-batch number 291000\n",
            "Training mini-batch number 292000\n",
            "Training mini-batch number 293000\n",
            "Training mini-batch number 294000\n",
            "Epoch 58: validation accuracy 0.9774\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.977\n",
            "Training mini-batch number 295000\n",
            "Training mini-batch number 296000\n",
            "Training mini-batch number 297000\n",
            "Training mini-batch number 298000\n",
            "Training mini-batch number 299000\n",
            "Epoch 59: validation accuracy 0.9774\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9771\n",
            "Finished training network.\n",
            "Best validation accuracy of 97.74% obtained at iteration 299999\n",
            "Corresponding test accuracy of 97.71%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net.test_mb_outputs(0)"
      ],
      "metadata": {
        "id": "5gWTjX21Hc_r",
        "outputId": "25edbb7f-b2ea-4a0d-d32f-c47a761b3834",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[9.4941344e-10, 2.0043240e-08, 6.3777730e-09, 7.3897230e-05,\n",
              "        7.0002982e-13, 4.2484619e-10, 6.9104038e-14, 9.9992591e-01,\n",
              "        1.4048526e-10, 1.7742997e-07],\n",
              "       [1.1442880e-09, 1.0207618e-06, 9.9999869e-01, 7.5046323e-08,\n",
              "        3.2618331e-12, 1.1687014e-07, 1.8918849e-08, 2.8681541e-13,\n",
              "        9.5244729e-08, 6.6735666e-14],\n",
              "       [2.9407490e-11, 9.9993843e-01, 2.1361566e-05, 5.7246456e-08,\n",
              "        1.2683747e-09, 3.4499479e-07, 1.9640252e-07, 4.6275363e-06,\n",
              "        3.5006127e-05, 2.6521407e-09],\n",
              "       [9.9995744e-01, 8.5963618e-09, 2.8882852e-05, 7.2819063e-08,\n",
              "        3.4229857e-09, 1.5564079e-07, 3.4061554e-06, 9.8313194e-06,\n",
              "        9.4787060e-11, 1.7868248e-07],\n",
              "       [2.0927943e-10, 2.5326278e-11, 1.3545604e-08, 5.1467194e-12,\n",
              "        9.9999893e-01, 2.6650470e-11, 7.2399187e-10, 5.4558677e-07,\n",
              "        2.3667350e-09, 5.1264618e-07],\n",
              "       [2.6837262e-11, 9.9955243e-01, 3.0474447e-07, 1.7022467e-07,\n",
              "        2.9974929e-09, 2.1680504e-09, 2.5266553e-10, 4.4606696e-04,\n",
              "        9.5494534e-07, 4.1649361e-08],\n",
              "       [5.4779163e-13, 7.1653079e-09, 6.3234730e-12, 4.1587939e-10,\n",
              "        9.9983042e-01, 1.1931422e-07, 1.7697948e-08, 2.0269253e-08,\n",
              "        1.6545162e-04, 3.9691517e-06],\n",
              "       [9.1918327e-11, 9.0800995e-06, 6.9726610e-07, 3.6812689e-05,\n",
              "        1.8148460e-04, 3.1243175e-07, 2.1402053e-09, 2.2018082e-06,\n",
              "        3.0506200e-08, 9.9976939e-01],\n",
              "       [2.3698746e-04, 3.5769997e-05, 1.0739247e-01, 1.6145864e-09,\n",
              "        1.0061597e-04, 1.4478646e-03, 8.8740474e-01, 3.4644199e-08,\n",
              "        3.3814718e-03, 3.8177589e-08],\n",
              "       [5.7770073e-11, 1.3488155e-12, 3.5601258e-13, 2.6611025e-07,\n",
              "        4.6039226e-05, 1.4870708e-09, 3.5376446e-14, 1.7467446e-06,\n",
              "        1.1152927e-06, 9.9995083e-01]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show theano"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzPpIsXuF6Eb",
        "outputId": "9ce1a1a6-ef8f-435e-af50-52b559ab63bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: Theano\n",
            "Version: 0.8.0\n",
            "Summary: Optimizing compiler for evaluating mathematical expressions on CPUs and GPUs.\n",
            "Home-page: http://deeplearning.net/software/theano/\n",
            "Author: LISA laboratory, University of Montreal\n",
            "Author-email: theano-dev@googlegroups.com\n",
            "License: BSD\n",
            "Location: /usr/local/lib/python3.8/dist-packages\n",
            "Requires: numpy, scipy, six\n",
            "Required-by: \n"
          ]
        }
      ]
    }
  ]
}